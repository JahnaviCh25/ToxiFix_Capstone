{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1476179,"sourceType":"datasetVersion","datasetId":866247},{"sourceId":2094393,"sourceType":"datasetVersion","datasetId":1255953},{"sourceId":2798066,"sourceType":"datasetVersion","datasetId":1709138},{"sourceId":4034417,"sourceType":"datasetVersion","datasetId":2390305},{"sourceId":11066508,"sourceType":"datasetVersion","datasetId":6895979}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"XLMRoberta, Mbert and Mdistilbert of multilingual \nSteps 1- 5 from preprocess to model fine tune.\n\nScript 1: Preprocessing (for XLM-RoBERTa, mBERT, DistilBERT Multi)\nDisk space issues so compressed the space in below script 1 and 2","metadata":{},"attachments":{}},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import (\n    XLMRobertaTokenizer,\n    BertTokenizer,\n    DistilBertTokenizer\n)\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nimport gc\n\n# --- Preprocessing ---\n\n# Load Jigsaw dataset\ndata_path = \"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv\"\ndf = pd.read_csv(data_path)\ndf = df.drop_duplicates(subset=[\"comment_text\"])\ndf = df[[\"comment_text\", \"toxic\"]].dropna()\n\n# Balance classes\ntoxic_df = df[df[\"toxic\"] == 1]\nnon_toxic_df = df[df[\"toxic\"] == 0].sample(n=len(toxic_df), random_state=42)\nbalanced_df = pd.concat([toxic_df, non_toxic_df]).sample(frac=1, random_state=42)\n\n# Split: 70% train, 15% val, 15% test\ntrain_df, temp_df = train_test_split(balanced_df, test_size=0.3, random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n\n# Rename 'toxic' to 'labels' for Trainer compatibility\ntrain_df = train_df.rename(columns={\"toxic\": \"labels\"})\nval_df = val_df.rename(columns={\"toxic\": \"labels\"})\ntest_df = test_df.rename(columns={\"toxic\": \"labels\"})\n\n# Convert to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# Tokenization function with optimization\ndef tokenize_dataset(dataset, tokenizer, max_length=32):  # Reduced from 64 to 32\n    def tokenize_function(examples):\n        return tokenizer(examples[\"comment_text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n    # Remove original text after tokenization to save space\n    tokenized = dataset.map(tokenize_function, batched=True, remove_columns=[\"comment_text\"])\n    return tokenized\n\n# Initialize tokenizers for multilingual models\nxlm_tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\nbert_multi_tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\ndistilbert_multi_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n\n# Tokenize datasets for each multilingual model\ntrain_xlm = tokenize_dataset(train_dataset, xlm_tokenizer)\nval_xlm = tokenize_dataset(val_dataset, xlm_tokenizer)\ntest_xlm = tokenize_dataset(test_dataset, xlm_tokenizer)\n\ntrain_bert_multi = tokenize_dataset(train_dataset, bert_multi_tokenizer)\nval_bert_multi = tokenize_dataset(val_dataset, bert_multi_tokenizer)\ntest_bert_multi = tokenize_dataset(test_dataset, bert_multi_tokenizer)\n\ntrain_distilbert_multi = tokenize_dataset(train_dataset, distilbert_multi_tokenizer)\nval_distilbert_multi = tokenize_dataset(val_dataset, distilbert_multi_tokenizer)\ntest_distilbert_multi = tokenize_dataset(test_dataset, distilbert_multi_tokenizer)\n\n# Save tokenized datasets with compression\ndef save_compressed(dataset, path):\n    dataset.save_to_disk(path, num_proc=1)  # Single process to avoid memory spikes\n    gc.collect()  # Clear memory after saving\n\nsave_compressed(train_xlm, \"/kaggle/working/preprocessed/train_xlm\")\nsave_compressed(val_xlm, \"/kaggle/working/preprocessed/val_xlm\")\nsave_compressed(test_xlm, \"/kaggle/working/preprocessed/test_xlm\")\n\nsave_compressed(train_bert_multi, \"/kaggle/working/preprocessed/train_bert_multi\")\nsave_compressed(val_bert_multi, \"/kaggle/working/preprocessed/val_bert_multi\")\nsave_compressed(test_bert_multi, \"/kaggle/working/preprocessed/test_bert_multi\")\n\nsave_compressed(train_distilbert_multi, \"/kaggle/working/preprocessed/train_distilbert_multi\")\nsave_compressed(val_distilbert_multi, \"/kaggle/working/preprocessed/val_distilbert_multi\")\nsave_compressed(test_distilbert_multi, \"/kaggle/working/preprocessed/test_distilbert_multi\")\n\nprint(\"Preprocessing completed. Tokenized datasets saved to /kaggle/working/preprocessed/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T00:23:51.384357Z","iopub.execute_input":"2025-03-20T00:23:51.384774Z","iopub.status.idle":"2025-03-20T00:26:01.592141Z","shell.execute_reply.started":"2025-03-20T00:23:51.384742Z","shell.execute_reply":"2025-03-20T00:26:01.591188Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e530700576d477898aef70c3fcb25b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc7371f211974c178c0925030b3af452"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b6c7f63c4ac47c29cbc5c01815ccb36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d127e3a23784221819da40db341b4bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e67a98b6c954002aeb9f806493dffee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3528837eb399431881b4ed63b261e1d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.72M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b05c1612770547a3bdc3e3880d127182"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ba5b59d11fb4ae58a907f9eaf558b25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"354a0e5ffec44d52908dabf35316fa16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68dbce37bb6c425a939e9f518537f006"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e8b43909f6e420c931d13a66279d124"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffa5a21e954449499c6a79f530407cb0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21411 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b39dc551bf2043578e6465ab3b082d84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4588 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e21f5e4d1d9749db9bfacc0217de481b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4589 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"256484308d184da7a7a5993e1d62e9de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21411 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05cfcaa9fec345e1b6380a4a11712b3c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4588 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15f3d4fbfcf842d7aa4ba69efe0b675b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4589 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"894dd8f83b074cfc8f3fb8c9fcb43866"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21411 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ddb3494bf2d445da037591c069a22b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4588 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12746be04c304315876b41ee13af72d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4589 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3742958c493049ea93b20b02470c9d6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/21411 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"240030a3c254469bac67ead12b8f3511"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/4588 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0749f97173c40f69cb89d4dec4c23aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/4589 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30483cafd0cb4224b5935a68200a14c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/21411 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f0a152bcdd441ed93d0cac9d0304cdf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/4588 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7872e6846aa4356ab3e553439d39cd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/4589 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06f370423d4848b6910a93c7a457a129"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/21411 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10a43b2652804150a359d6b63660df8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/4588 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10302f7b73374dbda9a1efb899c7cd2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/4589 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"136c870816604eb49b680e2c7b8771a3"}},"metadata":{}},{"name":"stdout","text":"Preprocessing completed. Tokenized datasets saved to /kaggle/working/preprocessed/\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Script 2: Fine-Tuning (for XLM-RoBERTa, mBERT, DistilBERT Multi)","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import (\n    XLMRobertaForSequenceClassification,\n    BertForSequenceClassification,\n    DistilBertForSequenceClassification,\n    XLMRobertaTokenizer,\n    BertTokenizer,\n    DistilBertTokenizer,\n    Trainer, TrainingArguments\n)\nfrom datasets import Dataset\nimport gc\n\n# Load tokenized datasets\ntrain_xlm = Dataset.load_from_disk(\"/kaggle/working/preprocessed/train_xlm\")\nval_xlm = Dataset.load_from_disk(\"/kaggle/working/preprocessed/val_xlm\")\ntrain_bert_multi = Dataset.load_from_disk(\"/kaggle/working/preprocessed/train_bert_multi\")\nval_bert_multi = Dataset.load_from_disk(\"/kaggle/working/preprocessed/val_bert_multi\")\ntrain_distilbert_multi = Dataset.load_from_disk(\"/kaggle/working/preprocessed/train_distilbert_multi\")\nval_distilbert_multi = Dataset.load_from_disk(\"/kaggle/working/preprocessed/val_distilbert_multi\")\n\n# Shared training arguments with minimal disk usage\nbase_training_args = TrainingArguments(\n    output_dir=\"/kaggle/working/models\",  # Single shared directory\n    eval_strategy=\"epoch\",\n    save_strategy=\"no\",  # Disable checkpointing to save space\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    load_best_model_at_end=False,  # Avoid extra model loading\n    report_to=\"none\"\n)\n\n# Function to fine-tune and save a model\ndef fine_tune_model(model, tokenizer, train_dataset, val_dataset, model_name):\n    trainer = Trainer(\n        model=model,\n        args=base_training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n    )\n    trainer.train()\n    # Save only the final model and tokenizer to the shared directory\n    model.save_pretrained(f\"/kaggle/working/models/{model_name}\")\n    tokenizer.save_pretrained(f\"/kaggle/working/models/{model_name}\")\n    # Clear memory\n    del trainer, model\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# Fine-tune Multilingual BERT (mBERT)\nbert_multi_model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-uncased\", num_labels=2)\nbert_multi_tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\nfine_tune_model(bert_multi_model, bert_multi_tokenizer, train_bert_multi, val_bert_multi, \"bert_multi_finetuned\")\ndel bert_multi_tokenizer  # Tokenizer can be reused later if needed\ngc.collect()\n\n# Fine-tune Multilingual DistilBERT\ndistilbert_multi_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-multilingual-cased\", num_labels=2)\ndistilbert_multi_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\nfine_tune_model(distilbert_multi_model, distilbert_multi_tokenizer, train_distilbert_multi, val_distilbert_multi, \"distilbert_multi_finetuned\")\ndel distilbert_multi_tokenizer\ngc.collect()\n\n# Fine-tune XLM-RoBERTa (Multilingual) - Uncomment if needed\n#xlm_model = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=2)\n#xlm_tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n#fine_tune_model(xlm_model, xlm_tokenizer, train_xlm, val_xlm, \"xlm_finetuned\")\n#del xlm_tokenizer\n#gc.collect()\n\nprint(\"Fine-tuning completed. Models saved to /kaggle/working/models/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T00:26:01.593281Z","iopub.execute_input":"2025-03-20T00:26:01.593892Z","iopub.status.idle":"2025-03-20T00:42:26.229530Z","shell.execute_reply.started":"2025-03-20T00:26:01.593861Z","shell.execute_reply":"2025-03-20T00:42:26.228436Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/672M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc11e0570a8b4a66975a04898f7a3fb6"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8031' max='8031' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8031/8031 09:29, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.326300</td>\n      <td>0.292156</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.260300</td>\n      <td>0.369104</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.192800</td>\n      <td>0.431704</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/542M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de3079f8130b40b3b6cd6d236b117b6d"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8031' max='8031' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8031/8031 06:12, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.323000</td>\n      <td>0.296008</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.235900</td>\n      <td>0.363014</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.178400</td>\n      <td>0.474485</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Fine-tuning completed. Models saved to /kaggle/working/models/\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom transformers import (\n    XLMRobertaForSequenceClassification,\n    BertForSequenceClassification,\n    DistilBertForSequenceClassification,\n    XLMRobertaTokenizer,\n    BertTokenizer,\n    DistilBertTokenizer,\n    Trainer, TrainingArguments\n)\nfrom datasets import Dataset\nimport gc\n\n# Load tokenized datasets\ntrain_xlm = Dataset.load_from_disk(\"/kaggle/working/preprocessed/train_xlm\")\nval_xlm = Dataset.load_from_disk(\"/kaggle/working/preprocessed/val_xlm\")\n\n# Shared training arguments with minimal disk usage\nbase_training_args = TrainingArguments(\n    output_dir=\"/kaggle/working/models\",  # Single shared directory\n    eval_strategy=\"epoch\",\n    save_strategy=\"no\",  # Disable checkpointing to save space\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    load_best_model_at_end=False,  # Avoid extra model loading\n    report_to=\"none\"\n)\n\n\n# Function to fine-tune and save a model\ndef fine_tune_model(model, tokenizer, train_dataset, val_dataset, model_name):\n    trainer = Trainer(\n        model=model,\n        args=base_training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n    )\n    trainer.train()\n    # Save only the final model and tokenizer to the shared directory\n    model.save_pretrained(f\"/kaggle/working/models/{model_name}\")\n    tokenizer.save_pretrained(f\"/kaggle/working/models/{model_name}\")\n    # Clear memory\n    del trainer, model\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# Fine-tune XLM-RoBERTa (Multilingual) - Uncomment if needed\nxlm_model = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=2)\nxlm_tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\nfine_tune_model(xlm_model, xlm_tokenizer, train_xlm, val_xlm, \"xlm_finetuned\")\ndel xlm_tokenizer\ngc.collect()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T00:42:26.231698Z","iopub.execute_input":"2025-03-20T00:42:26.232288Z","iopub.status.idle":"2025-03-20T00:54:58.860762Z","shell.execute_reply.started":"2025-03-20T00:42:26.232260Z","shell.execute_reply":"2025-03-20T00:54:58.859939Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5326accc909140dd9c94220a1e91e75c"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8031' max='8031' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8031/8031 12:20, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.348400</td>\n      <td>0.330514</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.290000</td>\n      <td>0.358369</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.234900</td>\n      <td>0.429709</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"Script 3: Evaluation (for XLM-RoBERTa, mBERT, DistilBERT Multi)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import (\n    XLMRobertaForSequenceClassification,\n    BertForSequenceClassification,\n    DistilBertForSequenceClassification,\n    Trainer, TrainingArguments\n)\nfrom datasets import Dataset\nfrom sklearn.metrics import (\n    accuracy_score, f1_score, precision_score, recall_score,\n    roc_auc_score, confusion_matrix\n)\nimport numpy as np\n\n# --- Evaluation ---\n\n# Compute metrics function (updated to include AUC and confusion matrix)\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)  # Predicted classes\n    probs = pred.predictions[:, 1]  # Probabilities for positive class (toxic)\n\n    # Standard metrics\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds)\n    precision = precision_score(labels, preds)\n    recall = recall_score(labels, preds)\n    \n    # AUC-ROC\n    auc = roc_auc_score(labels, probs)\n    \n    # Confusion matrix\n    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n    \n    return {\n        \"accuracy\": acc,\n        \"f1\": f1,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"auc\": auc,\n        \"true_positives\": tp,\n        \"true_negatives\": tn,\n        \"false_positives\": fp,\n        \"false_negatives\": fn\n    }\n\n# Load test datasets\ntest_xlm = Dataset.load_from_disk(\"/kaggle/working/preprocessed/test_xlm\")\ntest_bert_multi = Dataset.load_from_disk(\"/kaggle/working/preprocessed/test_bert_multi\")\ntest_distilbert_multi = Dataset.load_from_disk(\"/kaggle/working/preprocessed/test_distilbert_multi\")\n\n# Load fine-tuned models\nxlm_model = XLMRobertaForSequenceClassification.from_pretrained(\"/kaggle/working/models/xlm_finetuned\")\nbert_multi_model = BertForSequenceClassification.from_pretrained(\"/kaggle/working/models/bert_multi_finetuned\")\ndistilbert_multi_model = DistilBertForSequenceClassification.from_pretrained(\"/kaggle/working/models/distilbert_multi_finetuned\")\n\n# Evaluation training args\neval_args = TrainingArguments(\n    output_dir=\"/kaggle/working/models\",\n    eval_strategy=\"epoch\",\n    per_device_eval_batch_size=8,\n    report_to=\"none\"\n)\n\n# Evaluate XLM-RoBERTa (Multilingual)\nxlm_trainer = Trainer(\n    model=xlm_model,\n    args=eval_args,\n    eval_dataset=test_xlm,\n    compute_metrics=compute_metrics\n)\nxlm_eval = xlm_trainer.evaluate()\n\n# Evaluate Multilingual BERT (mBERT)\nbert_multi_trainer = Trainer(\n    model=bert_multi_model,\n    args=eval_args,\n    eval_dataset=test_bert_multi,\n    compute_metrics=compute_metrics\n)\nbert_multi_eval = bert_multi_trainer.evaluate()\n\n# Evaluate Multilingual DistilBERT\ndistilbert_multi_trainer = Trainer(\n    model=distilbert_multi_model,\n    args=eval_args,\n    eval_dataset=test_distilbert_multi,\n    compute_metrics=compute_metrics\n)\ndistilbert_multi_eval = distilbert_multi_trainer.evaluate()\n\n# Compile and save results\nresults = {\n    \"Model\": [\"XLM-RoBERTa\", \"mBERT\", \"DistilBERT (Multi)\"],\n    \"Accuracy\": [xlm_eval[\"eval_accuracy\"], bert_multi_eval[\"eval_accuracy\"], distilbert_multi_eval[\"eval_accuracy\"]],\n    \"F1\": [xlm_eval[\"eval_f1\"], bert_multi_eval[\"eval_f1\"], distilbert_multi_eval[\"eval_f1\"]],\n    \"Precision\": [xlm_eval[\"eval_precision\"], bert_multi_eval[\"eval_precision\"], distilbert_multi_eval[\"eval_precision\"]],\n    \"Recall\": [xlm_eval[\"eval_recall\"], bert_multi_eval[\"eval_recall\"], distilbert_multi_eval[\"eval_recall\"]],\n    \"AUC\": [xlm_eval[\"eval_auc\"], bert_multi_eval[\"eval_auc\"], distilbert_multi_eval[\"eval_auc\"]],\n    \"True Positives\": [xlm_eval[\"eval_true_positives\"], bert_multi_eval[\"eval_true_positives\"], distilbert_multi_eval[\"eval_true_positives\"]],\n    \"True Negatives\": [xlm_eval[\"eval_true_negatives\"], bert_multi_eval[\"eval_true_negatives\"], distilbert_multi_eval[\"eval_true_negatives\"]],\n    \"False Positives\": [xlm_eval[\"eval_false_positives\"], bert_multi_eval[\"eval_false_positives\"], distilbert_multi_eval[\"eval_false_positives\"]],\n    \"False Negatives\": [xlm_eval[\"eval_false_negatives\"], bert_multi_eval[\"eval_false_negatives\"], distilbert_multi_eval[\"eval_false_negatives\"]]\n}\n\nresults_df = pd.DataFrame(results)\nresults_df.to_csv(\"/kaggle/working/multilingual_model_performance_with_auc_cm.csv\", index=False)\nprint(results_df)\n\nprint(\"Evaluation completed. Results with AUC and confusion matrix saved to /kaggle/working/multilingual_model_performance_with_auc_cm.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T00:54:58.862052Z","iopub.execute_input":"2025-03-20T00:54:58.862286Z","iopub.status.idle":"2025-03-20T00:55:20.030675Z","shell.execute_reply.started":"2025-03-20T00:54:58.862267Z","shell.execute_reply":"2025-03-20T00:55:20.029677Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='574' max='574' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [574/574 00:07]\n    </div>\n    "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='574' max='574' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [574/574 00:07]\n    </div>\n    "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='574' max='574' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [574/574 00:04]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"                Model  Accuracy        F1  Precision    Recall       AUC  \\\n0         XLM-RoBERTa  0.901722  0.902275   0.902471  0.902080  0.963818   \n1               mBERT  0.904990  0.904260   0.916741  0.892114  0.964314   \n2  DistilBERT (Multi)  0.893223  0.893893   0.893506  0.894281  0.955340   \n\n   True Positives  True Negatives  False Positives  False Negatives  \n0            2082            2056              225              226  \n1            2059            2094              187              249  \n2            2064            2035              246              244  \nEvaluation completed. Results with AUC and confusion matrix saved to /kaggle/working/multilingual_model_performance_with_auc_cm.csv\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"Testing on 3 youtube comments datasets","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import (\n    XLMRobertaForSequenceClassification, XLMRobertaTokenizer,\n    BertForSequenceClassification, BertTokenizer,\n    DistilBertForSequenceClassification, DistilBertTokenizer\n)\nfrom datasets import Dataset\nimport os\n\n# Disk space check function (optional, for Kaggle monitoring)\ndef check_disk_space():\n    stat = os.statvfs(\"/kaggle/working\")\n    free_space = stat.f_frsize * stat.f_bavail / (1024 ** 3)  # GB\n    print(f\"Free disk space: {free_space:.2f} GB\")\n    return free_space\n\n# Step 1: Load Excel file and convert to CSV\nexcel_file_path = \"/kaggle/input/random-multi-youtube-comments-scrapped/Youtube comments scrapper_Testing File.xlsx\"\ncomments_df = pd.read_excel(excel_file_path)\n\n# Check for 'comment' column and adjust if necessary\nif \"comment\" not in comments_df.columns:\n    print(\"Column 'comment' not found. Available columns:\", comments_df.columns)\n    # Assuming the first column might be comments if misnamed\n    comments_df = comments_df.rename(columns={comments_df.columns[0]: \"comment\"})\nelse:\n    print(f\"Loaded Excel file from {excel_file_path} with {len(comments_df)} comments.\")\n\n# Step 2: Convert to Hugging Face Dataset\ncomments_dataset = Dataset.from_pandas(comments_df[[\"comment\"]])\n\n# Step 3: Tokenizers for the three fine-tuned multilingual models\nxlm_tokenizer = XLMRobertaTokenizer.from_pretrained(\"/kaggle/working/models/xlm_finetuned\")\nbert_multi_tokenizer = BertTokenizer.from_pretrained(\"/kaggle/working/models/bert_multi_finetuned\")\ndistilbert_multi_tokenizer = DistilBertTokenizer.from_pretrained(\"/kaggle/working/models/distilbert_multi_finetuned\")\n\n# Step 4: Tokenization function\ndef tokenize_dataset(dataset, tokenizer, max_length=32):  # Using 32 as per your multilingual preprocessing\n    def tokenize_function(examples):\n        return tokenizer(examples[\"comment\"], padding=\"max_length\", truncation=True, max_length=max_length)\n    return dataset.map(tokenize_function, batched=True)\n\n# Tokenize datasets for all three models\ncomments_xlm = tokenize_dataset(comments_dataset, xlm_tokenizer)\ncomments_bert_multi = tokenize_dataset(comments_dataset, bert_multi_tokenizer)\ncomments_distilbert_multi = tokenize_dataset(comments_dataset, distilbert_multi_tokenizer)\n\n# Step 5: Load the fine-tuned multilingual models\nxlm_model = XLMRobertaForSequenceClassification.from_pretrained(\"/kaggle/working/models/xlm_finetuned\")\nbert_multi_model = BertForSequenceClassification.from_pretrained(\"/kaggle/working/models/bert_multi_finetuned\")\ndistilbert_multi_model = DistilBertForSequenceClassification.from_pretrained(\"/kaggle/working/models/distilbert_multi_finetuned\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move models to device\nxlm_model.to(device)\nbert_multi_model.to(device)\ndistilbert_multi_model.to(device)\n\n# Step 6: Predict toxicity function\ndef predict_toxicity(model, tokenizer, dataset, device):\n    model.eval()\n    predictions = []\n    probabilities = []\n    comments = dataset[\"comment\"]\n    for comment in comments:\n        inputs = tokenizer(comment, return_tensors=\"pt\", padding=True, truncation=True, max_length=32)\n        inputs = {key: value.to(device) for key, value in inputs.items()}\n        with torch.no_grad():\n            outputs = model(**inputs)\n        probs = torch.softmax(outputs.logits, dim=-1)\n        pred = probs.argmax(-1).item()\n        toxic_prob = probs[0][1].item()\n        predictions.append(pred)\n        probabilities.append(toxic_prob)\n    return predictions, probabilities\n\n# Step 7: Get predictions for each model\nprint(\"Predicting with XLM-RoBERTa...\")\nxlm_preds, xlm_probs = predict_toxicity(xlm_model, xlm_tokenizer, comments_xlm, device)\n\nprint(\"Predicting with mBERT...\")\nbert_multi_preds, bert_multi_probs = predict_toxicity(bert_multi_model, bert_multi_tokenizer, comments_bert_multi, device)\n\nprint(\"Predicting with DistilBERT Multi...\")\ndistilbert_multi_preds, distilbert_multi_probs = predict_toxicity(distilbert_multi_model, distilbert_multi_tokenizer, comments_distilbert_multi, device)\n\n# Step 8: Combine and save results\nresults_df = pd.DataFrame({\n    \"Comment\": comments_df[\"comment\"],\n    \"XLMRoBERTa_Pred\": xlm_preds,\n    \"XLMRoBERTa_Toxic_Prob\": xlm_probs,\n    \"mBERT_Pred\": bert_multi_preds,\n    \"mBERT_Toxic_Prob\": bert_multi_probs,\n    \"DistilBERTMulti_Pred\": distilbert_multi_preds,\n    \"DistilBERTMulti_Toxic_Prob\": distilbert_multi_probs\n})\n\nresults_csv_path = \"/kaggle/working/combined_multilingual_model_predictions_RMYC.csv\"\nresults_df.to_csv(results_csv_path, index=False)\nprint(f\"Predictions saved to {results_csv_path}\")\nprint(results_df.head(10))  # Show first 10 rows for inspection\n\n# Optional: Check disk space\ncheck_disk_space()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T00:55:20.031495Z","iopub.execute_input":"2025-03-20T00:55:20.031798Z","iopub.status.idle":"2025-03-20T00:55:40.606423Z","shell.execute_reply.started":"2025-03-20T00:55:20.031773Z","shell.execute_reply":"2025-03-20T00:55:40.605658Z"}},"outputs":[{"name":"stdout","text":"Loaded Excel file from /kaggle/input/random-multi-youtube-comments-scrapped/Youtube comments scrapper_Testing File.xlsx with 563 comments.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/563 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2a7c5d3d17f4880b84d37d80682505b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/563 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d955e510711d44f989b4a44f3bb2f4a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/563 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98fc0a15da1041cea952e531b8d51885"}},"metadata":{}},{"name":"stdout","text":"Predicting with XLM-RoBERTa...\nPredicting with mBERT...\nPredicting with DistilBERT Multi...\nPredictions saved to /kaggle/working/combined_multilingual_model_predictions_RMYC.csv\n                                             Comment  XLMRoBERTa_Pred  \\\n0         Learning to speak is always useful. Great!                0   \n1  Vocal Exercises begin at 7:50 :   1. Raise arm...                0   \n2  Imagine someone being late and walking in at 8:49                1   \n3  4:01 \"Tempered with love, honesty is a great t...                0   \n4  \"If you wish people with love its really hard ...                0   \n5  i feel like youtube recommendations are person...                0   \n6                      I like his head it is shining                0   \n7  Very fantastic video.❤❤  I came across this vi...                0   \n8  0:13 Intro 0:33 7 deadly sins of speaking 2:45...                0   \n9  The trick is, whenever you're talking to peopl...                1   \n\n   XLMRoBERTa_Toxic_Prob  mBERT_Pred  mBERT_Toxic_Prob  DistilBERTMulti_Pred  \\\n0               0.000986           0          0.000996                     0   \n1               0.369682           0          0.003796                     0   \n2               0.810748           0          0.001095                     0   \n3               0.007137           1          0.664957                     0   \n4               0.020378           0          0.217671                     0   \n5               0.017703           0          0.002755                     0   \n6               0.012058           0          0.001867                     1   \n7               0.003207           0          0.002531                     0   \n8               0.010659           0          0.003346                     0   \n9               0.966903           1          0.972511                     1   \n\n   DistilBERTMulti_Toxic_Prob  \n0                    0.001629  \n1                    0.005779  \n2                    0.001932  \n3                    0.012051  \n4                    0.014270  \n5                    0.003759  \n6                    0.996503  \n7                    0.003326  \n8                    0.008738  \n9                    0.817320  \nFree disk space: 17.32 GB\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"17.3154296875"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import (\n    XLMRobertaForSequenceClassification, XLMRobertaTokenizer,\n    BertForSequenceClassification, BertTokenizer,\n    DistilBertForSequenceClassification, DistilBertTokenizer\n)\nfrom datasets import Dataset\nimport os\n\n# Disk space check function (optional, for Kaggle monitoring)\ndef check_disk_space():\n    stat = os.statvfs(\"/kaggle/working\")\n    free_space = stat.f_frsize * stat.f_bavail / (1024 ** 3)  # GB\n    print(f\"Free disk space: {free_space:.2f} GB\")\n    return free_space\n\n# Step 1: Load CSV file from Kaggle environment\ncsv_file_path = \"/kaggle/input/youtube-toxicity-data/youtoxic_english_1000.csv\"  # Replace with your actual CSV file path\ncomments_df = pd.read_csv(csv_file_path)\n\n# Check for 'comment' column and adjust if necessary\nif \"Text\" not in comments_df.columns:\n    print(\"Column 'comment' not found. Available columns:\", comments_df.columns)\n    # Assuming the first column might be comments if misnamed\n    comments_df = comments_df.rename(columns={comments_df.columns[0]: \"comment\"})\nelse:\n    print(f\"Loaded Excel file from {excel_file_path} with {len(comments_df)} comments.\")\n\n# Step 2: Convert to Hugging Face Dataset\ncomments_dataset = Dataset.from_pandas(comments_df[[\"Text\"]])\n\n# Step 3: Tokenizers for the three fine-tuned multilingual models\nxlm_tokenizer = XLMRobertaTokenizer.from_pretrained(\"/kaggle/working/models/xlm_finetuned\")\nbert_multi_tokenizer = BertTokenizer.from_pretrained(\"/kaggle/working/models/bert_multi_finetuned\")\ndistilbert_multi_tokenizer = DistilBertTokenizer.from_pretrained(\"/kaggle/working/models/distilbert_multi_finetuned\")\n\n# Step 4: Tokenization function\ndef tokenize_dataset(dataset, tokenizer, max_length=32):  # Using 32 as per your multilingual preprocessing\n    def tokenize_function(examples):\n        return tokenizer(examples[\"Text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n    return dataset.map(tokenize_function, batched=True)\n\n# Tokenize datasets for all three models\ncomments_xlm = tokenize_dataset(comments_dataset, xlm_tokenizer)\ncomments_bert_multi = tokenize_dataset(comments_dataset, bert_multi_tokenizer)\ncomments_distilbert_multi = tokenize_dataset(comments_dataset, distilbert_multi_tokenizer)\n\n# Step 5: Load the fine-tuned multilingual models\nxlm_model = XLMRobertaForSequenceClassification.from_pretrained(\"/kaggle/working/models/xlm_finetuned\")\nbert_multi_model = BertForSequenceClassification.from_pretrained(\"/kaggle/working/models/bert_multi_finetuned\")\ndistilbert_multi_model = DistilBertForSequenceClassification.from_pretrained(\"/kaggle/working/models/distilbert_multi_finetuned\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move models to device\nxlm_model.to(device)\nbert_multi_model.to(device)\ndistilbert_multi_model.to(device)\n\n# Step 6: Predict toxicity function\ndef predict_toxicity(model, tokenizer, dataset, device):\n    model.eval()\n    predictions = []\n    probabilities = []\n    comments = dataset[\"Text\"]\n    for comment in comments:\n        inputs = tokenizer(comment, return_tensors=\"pt\", padding=True, truncation=True, max_length=32)\n        inputs = {key: value.to(device) for key, value in inputs.items()}\n        with torch.no_grad():\n            outputs = model(**inputs)\n        probs = torch.softmax(outputs.logits, dim=-1)\n        pred = probs.argmax(-1).item()\n        toxic_prob = probs[0][1].item()\n        predictions.append(pred)\n        probabilities.append(toxic_prob)\n    return predictions, probabilities\n\n# Step 7: Get predictions for each model\nprint(\"Predicting with XLM-RoBERTa...\")\nxlm_preds, xlm_probs = predict_toxicity(xlm_model, xlm_tokenizer, comments_xlm, device)\n\nprint(\"Predicting with mBERT...\")\nbert_multi_preds, bert_multi_probs = predict_toxicity(bert_multi_model, bert_multi_tokenizer, comments_bert_multi, device)\n\nprint(\"Predicting with DistilBERT Multi...\")\ndistilbert_multi_preds, distilbert_multi_probs = predict_toxicity(distilbert_multi_model, distilbert_multi_tokenizer, comments_distilbert_multi, device)\n\n# Step 8: Combine and save results\nresults_df = pd.DataFrame({\n    \"Comment\": comments_df[\"Text\"],\n    \"XLMRoBERTa_Pred\": xlm_preds,\n    \"XLMRoBERTa_Toxic_Prob\": xlm_probs,\n    \"mBERT_Pred\": bert_multi_preds,\n    \"mBERT_Toxic_Prob\": bert_multi_probs,\n    \"DistilBERTMulti_Pred\": distilbert_multi_preds,\n    \"DistilBERTMulti_Toxic_Prob\": distilbert_multi_probs\n})\n\nresults_csv_path = \"/kaggle/working/combined_multilingual_model_predictions_YTD.csv\"\nresults_df.to_csv(results_csv_path, index=False)\nprint(f\"Predictions saved to {results_csv_path}\")\nprint(results_df.head(10))  # Show first 10 rows for inspection\n\n# Optional: Check disk space\ncheck_disk_space()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T00:55:40.607534Z","iopub.execute_input":"2025-03-20T00:55:40.607927Z","iopub.status.idle":"2025-03-20T00:56:12.198521Z","shell.execute_reply.started":"2025-03-20T00:55:40.607892Z","shell.execute_reply":"2025-03-20T00:56:12.197775Z"}},"outputs":[{"name":"stdout","text":"Loaded Excel file from /kaggle/input/random-multi-youtube-comments-scrapped/Youtube comments scrapper_Testing File.xlsx with 1000 comments.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01c9b76c2a76409fae06e35f442897f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82056d97032b4a188a6a9f06cccbff96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2af97a7b641e433582ef92080f898006"}},"metadata":{}},{"name":"stdout","text":"Predicting with XLM-RoBERTa...\nPredicting with mBERT...\nPredicting with DistilBERT Multi...\nPredictions saved to /kaggle/working/combined_multilingual_model_predictions_YTD.csv\n                                             Comment  XLMRoBERTa_Pred  \\\n0  If only people would just take a step back and...                0   \n1  Law enforcement is not trained to shoot to app...                1   \n2  \\nDont you reckon them 'black lives matter' ba...                1   \n3  There are a very large number of people who do...                0   \n4  The Arab dude is absolutely right, he should h...                1   \n5  here people his facebook is https://www.facebo...                1   \n6  Check out this you tube post. \"Black man goes ...                1   \n7  I would LOVE to see this pussy go to Staten Is...                1   \n8                        I agree with the protestor.                0   \n9   mike browns father was made to say that boooshit                1   \n\n   XLMRoBERTa_Toxic_Prob  mBERT_Pred  mBERT_Toxic_Prob  DistilBERTMulti_Pred  \\\n0               0.056271           0          0.003222                     0   \n1               0.973483           1          0.998818                     1   \n2               0.996697           1          0.990269                     1   \n3               0.025175           0          0.027755                     0   \n4               0.980950           1          0.987370                     1   \n5               0.962825           1          0.918311                     0   \n6               0.972133           1          0.984758                     1   \n7               0.999110           1          0.999227                     1   \n8               0.001102           0          0.000990                     0   \n9               0.999128           1          0.997693                     1   \n\n   DistilBERTMulti_Toxic_Prob  \n0                    0.016351  \n1                    0.995840  \n2                    0.999261  \n3                    0.131227  \n4                    0.996851  \n5                    0.006980  \n6                    0.991604  \n7                    0.997825  \n8                    0.001256  \n9                    0.998977  \nFree disk space: 17.32 GB\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"17.31519317626953"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import (\n    XLMRobertaForSequenceClassification, XLMRobertaTokenizer,\n    BertForSequenceClassification, BertTokenizer,\n    DistilBertForSequenceClassification, DistilBertTokenizer\n)\nfrom datasets import Dataset\nimport os\n\n# Disk space check function (optional, for Kaggle monitoring)\ndef check_disk_space():\n    stat = os.statvfs(\"/kaggle/working\")\n    free_space = stat.f_frsize * stat.f_bavail / (1024 ** 3)  # GB\n    print(f\"Free disk space: {free_space:.2f} GB\")\n    return free_space\n\n# Step 1: Load CSV file from Kaggle environment\ncsv_file_path = \"/kaggle/input/most-liked-comments-on-youtube/youtube_dataset.csv\"  # Replace with your actual CSV file path\ncomments_df = pd.read_csv(csv_file_path)\n\n# Check for 'comment' column and adjust if necessary\nif \"Comment\" not in comments_df.columns:\n    print(\"Column 'comment' not found. Available columns:\", comments_df.columns)\n    # Assuming the first column might be comments if misnamed\n    comments_df = comments_df.rename(columns={comments_df.columns[0]: \"comment\"})\nelse:\n    print(f\"Loaded Excel file from {excel_file_path} with {len(comments_df)} comments.\")\n\n# Step 2: Convert to Hugging Face Dataset\ncomments_dataset = Dataset.from_pandas(comments_df[[\"Comment\"]])\n\n# Step 3: Tokenizers for the three fine-tuned multilingual models\nxlm_tokenizer = XLMRobertaTokenizer.from_pretrained(\"/kaggle/working/models/xlm_finetuned\")\nbert_multi_tokenizer = BertTokenizer.from_pretrained(\"/kaggle/working/models/bert_multi_finetuned\")\ndistilbert_multi_tokenizer = DistilBertTokenizer.from_pretrained(\"/kaggle/working/models/distilbert_multi_finetuned\")\n\n# Step 4: Tokenization function\ndef tokenize_dataset(dataset, tokenizer, max_length=32):  # Using 32 as per your multilingual preprocessing\n    def tokenize_function(examples):\n        return tokenizer(examples[\"Comment\"], padding=\"max_length\", truncation=True, max_length=max_length)\n    return dataset.map(tokenize_function, batched=True)\n\n# Tokenize datasets for all three models\ncomments_xlm = tokenize_dataset(comments_dataset, xlm_tokenizer)\ncomments_bert_multi = tokenize_dataset(comments_dataset, bert_multi_tokenizer)\ncomments_distilbert_multi = tokenize_dataset(comments_dataset, distilbert_multi_tokenizer)\n\n# Step 5: Load the fine-tuned multilingual models\nxlm_model = XLMRobertaForSequenceClassification.from_pretrained(\"/kaggle/working/models/xlm_finetuned\")\nbert_multi_model = BertForSequenceClassification.from_pretrained(\"/kaggle/working/models/bert_multi_finetuned\")\ndistilbert_multi_model = DistilBertForSequenceClassification.from_pretrained(\"/kaggle/working/models/distilbert_multi_finetuned\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move models to device\nxlm_model.to(device)\nbert_multi_model.to(device)\ndistilbert_multi_model.to(device)\n\n# Step 6: Predict toxicity function\ndef predict_toxicity(model, tokenizer, dataset, device):\n    model.eval()\n    predictions = []\n    probabilities = []\n    comments = dataset[\"Comment\"]\n    for comment in comments:\n        inputs = tokenizer(comment, return_tensors=\"pt\", padding=True, truncation=True, max_length=32)\n        inputs = {key: value.to(device) for key, value in inputs.items()}\n        with torch.no_grad():\n            outputs = model(**inputs)\n        probs = torch.softmax(outputs.logits, dim=-1)\n        pred = probs.argmax(-1).item()\n        toxic_prob = probs[0][1].item()\n        predictions.append(pred)\n        probabilities.append(toxic_prob)\n    return predictions, probabilities\n\n# Step 7: Get predictions for each model\nprint(\"Predicting with XLM-RoBERTa...\")\nxlm_preds, xlm_probs = predict_toxicity(xlm_model, xlm_tokenizer, comments_xlm, device)\n\nprint(\"Predicting with mBERT...\")\nbert_multi_preds, bert_multi_probs = predict_toxicity(bert_multi_model, bert_multi_tokenizer, comments_bert_multi, device)\n\nprint(\"Predicting with DistilBERT Multi...\")\ndistilbert_multi_preds, distilbert_multi_probs = predict_toxicity(distilbert_multi_model, distilbert_multi_tokenizer, comments_distilbert_multi, device)\n\n# Step 8: Combine and save results\nresults_df = pd.DataFrame({\n    \"Comment\": comments_df[\"Comment\"],\n    \"XLMRoBERTa_Pred\": xlm_preds,\n    \"XLMRoBERTa_Toxic_Prob\": xlm_probs,\n    \"mBERT_Pred\": bert_multi_preds,\n    \"mBERT_Toxic_Prob\": bert_multi_probs,\n    \"DistilBERTMulti_Pred\": distilbert_multi_preds,\n    \"DistilBERTMulti_Toxic_Prob\": distilbert_multi_probs\n})\n\nresults_csv_path = \"/kaggle/working/combined_multilingual_model_predictions_YD.csv\"\nresults_df.to_csv(results_csv_path, index=False)\nprint(f\"Predictions saved to {results_csv_path}\")\nprint(results_df.head(10))  # Show first 10 rows for inspection\n\n# Optional: Check disk space\ncheck_disk_space()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T00:56:12.199467Z","iopub.execute_input":"2025-03-20T00:56:12.199843Z","iopub.status.idle":"2025-03-20T01:04:11.305406Z","shell.execute_reply.started":"2025-03-20T00:56:12.199806Z","shell.execute_reply":"2025-03-20T01:04:11.304377Z"}},"outputs":[{"name":"stdout","text":"Loaded Excel file from /kaggle/input/random-multi-youtube-comments-scrapped/Youtube comments scrapper_Testing File.xlsx with 19300 comments.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea4fa3301c294d84880b8559a2657bf5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ed3f7d341bb4e008f683c7414dbebc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da3b6720e5884fedb7b5710244ee15ed"}},"metadata":{}},{"name":"stdout","text":"Predicting with XLM-RoBERTa...\nPredicting with mBERT...\nPredicting with DistilBERT Multi...\nPredictions saved to /kaggle/working/combined_multilingual_model_predictions_YD.csv\n                                             Comment  XLMRoBERTa_Pred  \\\n0  The people who liked this comment is officiall...                0   \n1           - Wait, it's 7B views\\n- Always has been                0   \n2  *Teacher: What is the population of the Earth?...                0   \n3  Let's be honest this wasn't your recommendatio...                0   \n4  Types Of People:\\n10% Enjoying Song \\n90% Chec...                0   \n5  3.2 Million comments if you find mine your a l...                1   \n6  claim your “here before 7 billion” tickets her...                0   \n7  The ones who are NOT from Tik-Tok can like thi...                0   \n8   Song: spanish\\nComments: English\\nHotel: trivago                0   \n9                                    Kimler burda😂🥰🌹                1   \n\n   XLMRoBERTa_Toxic_Prob  mBERT_Pred  mBERT_Toxic_Prob  DistilBERTMulti_Pred  \\\n0               0.001349           0          0.001374                     0   \n1               0.002301           0          0.002890                     0   \n2               0.001617           0          0.001632                     0   \n3               0.003461           0          0.001128                     0   \n4               0.001240           0          0.001166                     0   \n5               0.980106           0          0.004539                     0   \n6               0.003825           0          0.001784                     0   \n7               0.001181           0          0.003887                     0   \n8               0.001322           0          0.001042                     0   \n9               0.981423           0          0.000849                     0   \n\n   DistilBERTMulti_Toxic_Prob  \n0                    0.003587  \n1                    0.001897  \n2                    0.003717  \n3                    0.001574  \n4                    0.001375  \n5                    0.002552  \n6                    0.002480  \n7                    0.003653  \n8                    0.001330  \n9                    0.022350  \nFree disk space: 17.31 GB\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"17.311519622802734"},"metadata":{}}],"execution_count":7}]}