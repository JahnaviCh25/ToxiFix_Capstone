{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1476179,"sourceType":"datasetVersion","datasetId":866247},{"sourceId":2094393,"sourceType":"datasetVersion","datasetId":1255953},{"sourceId":2798066,"sourceType":"datasetVersion","datasetId":1709138},{"sourceId":4034417,"sourceType":"datasetVersion","datasetId":2390305},{"sourceId":11066508,"sourceType":"datasetVersion","datasetId":6895979}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Step 1: Preprocess Jigsaw Dataset (Clean, Balance, Split, Tokenize)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer, RobertaTokenizer, DistilBertTokenizer\nfrom datasets import Dataset\n\n# Load dataset from Kaggle input\ndata_path = \"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv\"\ndf = pd.read_csv(data_path)\n\n# Clean dataset\ndf = df.drop_duplicates(subset=[\"comment_text\"])\ndf = df[[\"comment_text\", \"toxic\"]].dropna()\n\n# Balance classes (optional)\ntoxic_df = df[df[\"toxic\"] == 1]\nnon_toxic_df = df[df[\"toxic\"] == 0].sample(n=len(toxic_df), random_state=42)\nbalanced_df = pd.concat([toxic_df, non_toxic_df]).sample(frac=1, random_state=42)\n\n# Split: 70% train, 15% val, 15% test\ntrain_df, temp_df = train_test_split(balanced_df, test_size=0.3, random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n\n# Rename 'toxic' to 'labels' for Trainer compatibility\ntrain_df = train_df.rename(columns={\"toxic\": \"labels\"})\nval_df = val_df.rename(columns={\"toxic\": \"labels\"})\ntest_df = test_df.rename(columns={\"toxic\": \"labels\"})\n\n# Convert to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# Tokenization function\ndef tokenize_dataset(dataset, tokenizer, max_length=64):\n    def tokenize_function(examples):\n        return tokenizer(examples[\"comment_text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n    return dataset.map(tokenize_function, batched=True)\n\n# Initialize tokenizers\nbert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nroberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\ndistilbert_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\n# Tokenize for each model\ntrain_bert = tokenize_dataset(train_dataset, bert_tokenizer)\nval_bert = tokenize_dataset(val_dataset, bert_tokenizer)\ntest_bert = tokenize_dataset(test_dataset, bert_tokenizer)\n\ntrain_roberta = tokenize_dataset(train_dataset, roberta_tokenizer)\nval_roberta = tokenize_dataset(val_dataset, roberta_tokenizer)\ntest_roberta = tokenize_dataset(test_dataset, roberta_tokenizer)\n\ntrain_distilbert = tokenize_dataset(train_dataset, distilbert_tokenizer)\nval_distilbert = tokenize_dataset(val_dataset, distilbert_tokenizer)\ntest_distilbert = tokenize_dataset(test_dataset, distilbert_tokenizer)\n\n# Save to Kaggle working directory\ntrain_bert.save_to_disk(\"/kaggle/working/preprocessed/train_bert\")\nval_bert.save_to_disk(\"/kaggle/working/preprocessed/val_bert\")\ntest_bert.save_to_disk(\"/kaggle/working/preprocessed/test_bert\")\ntrain_roberta.save_to_disk(\"/kaggle/working/preprocessed/train_roberta\")\nval_roberta.save_to_disk(\"/kaggle/working/preprocessed/val_roberta\")\ntest_roberta.save_to_disk(\"/kaggle/working/preprocessed/test_roberta\")\ntrain_distilbert.save_to_disk(\"/kaggle/working/preprocessed/train_distilbert\")\nval_distilbert.save_to_disk(\"/kaggle/working/preprocessed/val_distilbert\")\ntest_distilbert.save_to_disk(\"/kaggle/working/preprocessed/test_distilbert\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Step 2: Fine-Tune BERT, DistilBERT, RoBERTa with Validation Set","metadata":{}},{"cell_type":"code","source":"from transformers import (BertForSequenceClassification, RobertaForSequenceClassification, \n                          DistilBertForSequenceClassification, Trainer, TrainingArguments)\nimport torch\n\n# Load preprocessed datasets\ntrain_bert = Dataset.load_from_disk(\"/kaggle/working/preprocessed/train_bert\")\nval_bert = Dataset.load_from_disk(\"/kaggle/working/preprocessed/val_bert\")\ntrain_roberta = Dataset.load_from_disk(\"/kaggle/working/preprocessed/train_roberta\")\nval_roberta = Dataset.load_from_disk(\"/kaggle/working/preprocessed/val_roberta\")\ntrain_distilbert = Dataset.load_from_disk(\"/kaggle/working/preprocessed/train_distilbert\")\nval_distilbert = Dataset.load_from_disk(\"/kaggle/working/preprocessed/val_distilbert\")\n\n# Load models\nbert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\nroberta_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\ndistilbert_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/models\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    report_to=\"none\"\n)\n\n# Fine-tune function\ndef fine_tune_model(model, train_dataset, val_dataset, model_name):\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n    )\n    trainer.train()\n    trainer.save_model(f\"/kaggle/working/models/{model_name}\")\n    return trainer\n\n# Fine-tune each model\nbert_trainer = fine_tune_model(bert_model, train_bert, val_bert, \"bert_finetuned\")\nroberta_trainer = fine_tune_model(roberta_model, train_roberta, val_roberta, \"roberta_finetuned\")\ndistilbert_trainer = fine_tune_model(distilbert_model, train_distilbert, val_distilbert, \"distilbert_finetuned\")\n\n# Save tokenizers\nbert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nroberta_tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\ndistilbert_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\nbert_tokenizer.save_pretrained(\"/kaggle/working/models/bert_finetuned\")\nroberta_tokenizer.save_pretrained(\"/kaggle/working/models/roberta_finetuned\")\ndistilbert_tokenizer.save_pretrained(\"/kaggle/working/models/distilbert_finetuned\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Step 3: Evaluate on Test Set and Document Results","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nimport pandas as pd\nfrom transformers import BertForSequenceClassification, RobertaForSequenceClassification, DistilBertForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import Dataset\n\n# Load test datasets\ntest_bert = Dataset.load_from_disk(\"/kaggle/working/preprocessed/test_bert\")\ntest_roberta = Dataset.load_from_disk(\"/kaggle/working/preprocessed/test_roberta\")\ntest_distilbert = Dataset.load_from_disk(\"/kaggle/working/preprocessed/test_distilbert\")\n\n# Load fine-tuned models\nbert_model = BertForSequenceClassification.from_pretrained(\"/kaggle/working/models/bert_finetuned\")\nroberta_model = RobertaForSequenceClassification.from_pretrained(\"/kaggle/working/models/roberta_finetuned\")\ndistilbert_model = DistilBertForSequenceClassification.from_pretrained(\"/kaggle/working/models/distilbert_finetuned\")\n\n# Define training args for evaluation (no training, just eval)\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/models\",\n    eval_strategy=\"epoch\",  # Still works since we provide eval_dataset\n    per_device_eval_batch_size=8,\n    report_to=\"none\"\n)\n\n# Compute metrics\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = pred.predictions.argmax(-1)\n    acc = accuracy_score(labels, preds)\n    f1 = f1_score(labels, preds)\n    precision = precision_score(labels, preds)\n    recall = recall_score(labels, preds)\n    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n\n# Evaluate each model with its test dataset\nbert_trainer = Trainer(\n    model=bert_model,\n    args=training_args,\n    eval_dataset=test_bert,  # Pass test set as eval_dataset\n    compute_metrics=compute_metrics\n)\nroberta_trainer = Trainer(\n    model=roberta_model,\n    args=training_args,\n    eval_dataset=test_roberta,  # Pass test set as eval_dataset\n    compute_metrics=compute_metrics\n)\ndistilbert_trainer = Trainer(\n    model=distilbert_model,\n    args=training_args,\n    eval_dataset=test_distilbert,  # Pass test set as eval_dataset\n    compute_metrics=compute_metrics\n)\n\n# Run evaluation\nbert_eval = bert_trainer.evaluate()\nroberta_eval = roberta_trainer.evaluate()\ndistilbert_eval = distilbert_trainer.evaluate()\n\n# Document results\nresults = {\n    \"Model\": [\"BERT\", \"RoBERTa\", \"DistilBERT\"],\n    \"Accuracy\": [bert_eval[\"eval_accuracy\"], roberta_eval[\"eval_accuracy\"], distilbert_eval[\"eval_accuracy\"]],\n    \"F1\": [bert_eval[\"eval_f1\"], roberta_eval[\"eval_f1\"], distilbert_eval[\"eval_f1\"]],\n    \"Precision\": [bert_eval[\"eval_precision\"], roberta_eval[\"eval_precision\"], distilbert_eval[\"eval_precision\"]],\n    \"Recall\": [bert_eval[\"eval_recall\"], roberta_eval[\"eval_recall\"], distilbert_eval[\"eval_recall\"]]\n}\nresults_df = pd.DataFrame(results)\nresults_df.to_csv(\"/kaggle/working/model_performance.csv\", index=False)\nprint(results_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Step 4: Test on Curated Toxic/Non-Toxic Examples","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, RobertaTokenizer, DistilBertTokenizer\n\n# Load tokenizers\nbert_tokenizer = BertTokenizer.from_pretrained(\"/kaggle/working/models/bert_finetuned\")\nroberta_tokenizer = RobertaTokenizer.from_pretrained(\"/kaggle/working/models/roberta_finetuned\")\ndistilbert_tokenizer = DistilBertTokenizer.from_pretrained(\"/kaggle/working/models/distilbert_finetuned\")\n\n# Move models to GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbert_model.to(device)\nroberta_model.to(device)\ndistilbert_model.to(device)\n\n# Curated examples\nexamples = [\n    {\"text\": \"You’re a complete idiot!\", \"label\": 1},\n    {\"text\": \"Nice work, well done!\", \"label\": 0},\n    {\"text\": \"Die, you worthless scum.\", \"label\": 1},\n    {\"text\": \"This is a great discussion.\", \"label\": 0},\n]\n\n# Prediction function (ensure inputs are on the same device as model)\ndef predict_toxicity(model, tokenizer, text, device):\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n    inputs = {key: value.to(device) for key, value in inputs.items()}  # Move inputs to GPU\n    with torch.no_grad():  # Disable gradient computation for inference\n        outputs = model(**inputs)\n    probs = torch.softmax(outputs.logits, dim=-1)\n    pred = probs.argmax(-1).item()\n    return pred, probs[0][1].item()\n\n# Test each model\nfor example in examples:\n    text, true_label = example[\"text\"], example[\"label\"]\n    print(f\"\\nText: {text}\")\n    for model_name, model, tokenizer in [\n        (\"BERT\", bert_model, bert_tokenizer),\n        (\"RoBERTa\", roberta_model, roberta_tokenizer),\n        (\"DistilBERT\", distilbert_model, distilbert_tokenizer),\n    ]:\n        pred, prob = predict_toxicity(model, tokenizer, text, device)\n        print(f\"{model_name}: Predicted={pred}, Toxic Prob={prob:.4f}, True={true_label}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Step 5: Prepare Models for Web App (Test Inference, Optimize)","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\nimport time\n\n# Use DistilBERT for inference\nclassifier = pipeline(\"text-classification\", model=\"/kaggle/working/models/distilbert_finetuned\", \n                      tokenizer=distilbert_tokenizer)\n\n# Test inference\nstart_time = time.time()\nresult = classifier(\"You’re a disgusting human being!\")\nend_time = time.time()\nprint(f\"Prediction: {result}, Latency: {end_time - start_time:.4f}s\")\n\n# Additional test for confirmation\nstart_time = time.time()\nresult = classifier(\"Great job, keep it up!\")\nend_time = time.time()\nprint(f\"Prediction: {result}, Latency: {end_time - start_time:.4f}s\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Testing with Youtube Comments","metadata":{}},{"cell_type":"code","source":"#Testing on youtube comments datasets\n\nimport pandas as pd\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification\nfrom datasets import Dataset\n\n# Step 1: Load CSV file from Kaggle environment\ncsv_file_path = \"/kaggle/input/youtube-toxicity-data/youtoxic_english_1000.csv\"  # Replace with your actual CSV file path\ncomments_df = pd.read_csv(csv_file_path)\n\n# Ensure the column name matches your CSV file (adjust if different)\nif \"Text\" not in comments_df.columns:\n    print(\"Column 'comment' not found. Available columns:\", comments_df.columns)\nelse:\n    print(f\"Loaded CSV from {csv_file_path} with {len(comments_df)} comments.\")\n\n# Step 2: Convert to Hugging Face Dataset\ncomments_dataset = Dataset.from_pandas(comments_df[[\"Text\"]])\n\n# Step 3: Tokenizers for the three fine-tuned models\nbert_tokenizer = BertTokenizer.from_pretrained(\"/kaggle/working/models/bert_finetuned\")\nroberta_tokenizer = RobertaTokenizer.from_pretrained(\"/kaggle/working/models/roberta_finetuned\")\ndistilbert_tokenizer = DistilBertTokenizer.from_pretrained(\"/kaggle/working/models/distilbert_finetuned\")\n\n# Step 4: Tokenization function\ndef tokenize_dataset(dataset, tokenizer, max_length=64):\n    def tokenize_function(examples):\n        return tokenizer(examples[\"Text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n    return dataset.map(tokenize_function, batched=True)\n\n# Tokenize datasets for all three models\ncomments_bert = tokenize_dataset(comments_dataset, bert_tokenizer)\ncomments_roberta = tokenize_dataset(comments_dataset, roberta_tokenizer)\ncomments_distilbert = tokenize_dataset(comments_dataset, distilbert_tokenizer)\n\n# Step 5: Load the models\nbert_model = BertForSequenceClassification.from_pretrained(\"/kaggle/working/models/bert_finetuned\")\nroberta_model = RobertaForSequenceClassification.from_pretrained(\"/kaggle/working/models/roberta_finetuned\")\ndistilbert_model = DistilBertForSequenceClassification.from_pretrained(\"/kaggle/working/models/distilbert_finetuned\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move models to device\nbert_model.to(device)\nroberta_model.to(device)\ndistilbert_model.to(device)\n\n# Step 6: Predict toxicity function\ndef predict_toxicity(model, tokenizer, dataset, device):\n    model.eval()\n    predictions = []\n    probabilities = []\n    comments = dataset[\"Text\"]\n    for comment in comments:\n        inputs = tokenizer(comment, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n        inputs = {key: value.to(device) for key, value in inputs.items()}\n        with torch.no_grad():\n            outputs = model(**inputs)\n        probs = torch.softmax(outputs.logits, dim=-1)\n        pred = probs.argmax(-1).item()\n        toxic_prob = probs[0][1].item()\n        predictions.append(pred)\n        probabilities.append(toxic_prob)\n    return predictions, probabilities\n\n# Step 7: Get predictions for each model\nprint(\"Predicting with BERT...\")\nbert_preds, bert_probs = predict_toxicity(bert_model, bert_tokenizer, comments_bert, device)\n\nprint(\"Predicting with RoBERTa...\")\nroberta_preds, roberta_probs = predict_toxicity(roberta_model, roberta_tokenizer, comments_roberta, device)\n\nprint(\"Predicting with DistilBERT...\")\ndistilbert_preds, distilbert_probs = predict_toxicity(distilbert_model, distilbert_tokenizer, comments_distilbert, device)\n\n# Step 8: Combine and save results\nresults_df = pd.DataFrame({\n    \"Comment\": comments_df[\"Text\"],\n    \"BERT_Pred\": bert_preds,\n    \"BERT_Toxic_Prob\": bert_probs,\n    \"RoBERTa_Pred\": roberta_preds,\n    \"RoBERTa_Toxic_Prob\": roberta_probs,\n    \"DistilBERT_Pred\": distilbert_preds,\n    \"DistilBERT_Toxic_Prob\": distilbert_probs\n})\n\nresults_csv_path = \"/kaggle/working/combined_model_predictions_YTD.csv\"\nresults_df.to_csv(results_csv_path, index=False)\nprint(f\"Predictions saved to {results_csv_path}\")\nprint(results_df.head(10))  # Show first 10 rows for inspection\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}