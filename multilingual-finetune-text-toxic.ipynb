{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1476179,"sourceType":"datasetVersion","datasetId":866247},{"sourceId":2094393,"sourceType":"datasetVersion","datasetId":1255953},{"sourceId":2798066,"sourceType":"datasetVersion","datasetId":1709138},{"sourceId":4034417,"sourceType":"datasetVersion","datasetId":2390305},{"sourceId":11066508,"sourceType":"datasetVersion","datasetId":6895979}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Step 1: Fine-Tune XLM-RoBERTa and Test All Models on YouTube Comments\n\nimport torch\nimport pandas as pd\nfrom transformers import (XLMRobertaTokenizer, XLMRobertaForSequenceClassification, \n                          BertForSequenceClassification, BertTokenizer,\n                          RobertaForSequenceClassification, RobertaTokenizer,\n                          DistilBertForSequenceClassification, DistilBertTokenizer,\n                          Trainer, TrainingArguments)\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\n\n# --- Part 1: Fine-Tune XLM-RoBERTa ---\n\n# Load Jigsaw dataset\ndata_path = \"/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv\"\ndf = pd.read_csv(data_path)\ndf = df.drop_duplicates(subset=[\"comment_text\"])\ndf = df[[\"comment_text\", \"toxic\"]].dropna()\n\n# Balance classes\ntoxic_df = df[df[\"toxic\"] == 1]\nnon_toxic_df = df[df[\"toxic\"] == 0].sample(n=len(toxic_df), random_state=42)\nbalanced_df = pd.concat([toxic_df, non_toxic_df]).sample(frac=1, random_state=42)\n\n# Split: 70% train, 15% val, 15% test\ntrain_df, temp_df = train_test_split(balanced_df, test_size=0.3, random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n\n# Rename 'toxic' to 'labels' for Trainer compatibility\ntrain_df = train_df.rename(columns={\"toxic\": \"labels\"})\nval_df = val_df.rename(columns={\"toxic\": \"labels\"})\ntest_df = test_df.rename(columns={\"toxic\": \"labels\"})\n\n# Convert to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# Tokenize with XLM-RoBERTa\nxlm_tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\ndef tokenize_dataset(dataset, tokenizer, max_length=64):\n    def tokenize_function(examples):\n        return tokenizer(examples[\"comment_text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n    return dataset.map(tokenize_function, batched=True)\n\ntrain_xlm = tokenize_dataset(train_dataset, xlm_tokenizer)\nval_xlm = tokenize_dataset(val_dataset, xlm_tokenizer)\ntest_xlm = tokenize_dataset(test_dataset, xlm_tokenizer)\n\n# Save tokenized datasets\ntrain_xlm.save_to_disk(\"/kaggle/working/preprocessed/train_xlm\")\nval_xlm.save_to_disk(\"/kaggle/working/preprocessed/val_xlm\")\ntest_xlm.save_to_disk(\"/kaggle/working/preprocessed/test_xlm\")\n\n# Fine-tune XLM-RoBERTa\nxlm_model = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=2)\ntraining_args = TrainingArguments(\n    output_dir=\"/kaggle/working/models/xlm_finetuned\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    report_to=\"none\"\n)\n\ntrainer = Trainer(\n    model=xlm_model,\n    args=training_args,\n    train_dataset=train_xlm,\n    eval_dataset=val_xlm,\n)\ntrainer.train()\ntrainer.save_model(\"/kaggle/working/models/xlm_finetuned\")\nxlm_tokenizer.save_pretrained(\"/kaggle/working/models/xlm_finetuned\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T21:56:25.267018Z","iopub.execute_input":"2025-03-17T21:56:25.267305Z","iopub.status.idle":"2025-03-17T22:12:33.015682Z","shell.execute_reply.started":"2025-03-17T21:56:25.267275Z","shell.execute_reply":"2025-03-17T22:12:33.014767Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09d8040d49704d3ab79b7af0ae7a5423"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b233fec0cceb4a98a09e01909f3b30cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91a7140d477d4ccd8a1cab803003912c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a80af5efbd349d8b0277d2b2196bf67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/21411 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14d04ae369f44b7c86d8147e675e6907"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4588 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caac0c737a2640f9af73782a5cd85b2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4589 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddfbd08c46b24b0cb0aa5c37633de265"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/21411 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b83be80d094d4e539a19ce0031011582"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/4588 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae3ea756cace44c98639ec676569eaa2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/4589 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e884d9429f294e0e95f567ca1620300f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"904f3865f4954e1eb140d9eb62c81992"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='8031' max='8031' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8031/8031 15:12, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.282000</td>\n      <td>0.249782</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.220200</td>\n      <td>0.338507</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.172300</td>\n      <td>0.349726</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/models/xlm_finetuned/tokenizer_config.json',\n '/kaggle/working/models/xlm_finetuned/special_tokens_map.json',\n '/kaggle/working/models/xlm_finetuned/sentencepiece.bpe.model',\n '/kaggle/working/models/xlm_finetuned/added_tokens.json')"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"\n# Test inference on multilingual examples\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nxlm_model = XLMRobertaForSequenceClassification.from_pretrained(\"/kaggle/working/models/xlm_finetuned\").to(device)\n\ndef predict_toxicity(model, tokenizer, text, device):\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n    with torch.no_grad():\n        outputs = model(**inputs)\n    probs = torch.softmax(outputs.logits, dim=-1)\n    pred = probs.argmax(-1).item()\n    return pred, probs[0][1].item()\n\n# Multilingual and emoji examples\nexamples = [\n    {\"text\": \"¬°Eres un idiota! üòÇ\", \"label\": 1},  # Spanish\n    {\"text\": \"Tu es g√©nial! üëç\", \"label\": 0},     # French\n    {\"text\": \"‡§Ü‡§™ ‡§Æ‡•Ç‡§∞‡•ç‡§ñ ‡§π‡•à‡§Ç! üò°\", \"label\": 1},     # Hindi\n    {\"text\": \"Great work üòä\", \"label\": 0},        # English with emoji\n]\n\nfor example in examples:\n    text, true_label = example[\"text\"], example[\"label\"]\n    pred, prob = predict_toxicity(xlm_model, xlm_tokenizer, text, device)\n    print(f\"\\nText: {text}\")\n    print(f\"XLM-RoBERTa: Predicted={pred}, Toxic Prob={prob:.4f}, True={true_label}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 6: Test XLM-RoBERTa on Pre-Collected YouTube Comments\n\nimport torch\nimport pandas as pd\nfrom transformers import XLMRobertaForSequenceClassification, XLMRobertaTokenizer\n\n# Load fine-tuned XLM-RoBERTa model and tokenizer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nxlm_model = XLMRobertaForSequenceClassification.from_pretrained(\"/kaggle/working/models/xlm_finetuned\").to(device)\nxlm_tokenizer = XLMRobertaTokenizer.from_pretrained(\"/kaggle/working/models/xlm_finetuned\")\n\n# Prediction function\ndef predict_toxicity(model, tokenizer, text, device):\n    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n    with torch.no_grad():\n        outputs = model(**inputs)\n    probs = torch.softmax(outputs.logits, dim=-1)\n    pred = probs.argmax(-1).item()\n    toxic_prob = probs[0][1].item()\n    return pred, toxic_prob\n\n# Pre-collected YouTube comments (curated to reflect diversity)\nyoutube_comments = [\n    # English without emojis/symbols\n    \"This video is terrible\",\n    \"Really well made content\",\n    \"You should quit making videos\",\n    \"Thanks for the great upload\",\n    # English with emojis/symbols\n    \"Worst thing ever üòÇüëé\",\n    \"Love this so much! üòç‚ú®\",\n    \"Go jump off a cliff ‚ò†Ô∏è\",\n    \"Awesome work! üòäüëç\",\n    # Multilingual without emojis/symbols\n    \"¬°Qu√© video tan malo!\",\n    \"Bon contenu, merci!\",\n    \"‡§Ü‡§™‡§ï‡§æ ‡§µ‡•Ä‡§°‡§ø‡§Ø‡•ã ‡§∂‡§æ‡§®‡§¶‡§æ‡§∞ ‡§π‡•à\",\n    \"„Åì„ÅÆ„Éì„Éá„Ç™„ÅØ„Å≤„Å©„ÅÑ\",\n    # Multilingual with emojis/symbols\n    \"¬°Eres un genio! ‚≠ê\",\n    \"Tu es nul üò°üëä\",\n    \"‡§¨‡§π‡•Å‡§§ ‡§¨‡•á‡§ï‡§æ‡§∞! üò§üí¢\",\n    \"Á¥†Êô¥„Çâ„Åó„ÅÑ„Å≠ üòäüëç\"\n]\n\nprint(f\"Testing {len(youtube_comments)} YouTube comments with XLM-RoBERTa\")\n\n# Test XLM-RoBERTa and store results\nresults = []\nfor comment in youtube_comments:\n    true_label = -1  # Unknown, unless you annotate manually\n    xlm_pred, xlm_prob = predict_toxicity(xlm_model, xlm_tokenizer, comment, device)\n    \n    results.append({\n        \"Text\": comment,\n        \"True_Label\": true_label,\n        \"XLM_RoBERTa_Pred\": xlm_pred,\n        \"XLM_RoBERTa_Toxic_Prob\": xlm_prob\n    })\n\n# Convert to DataFrame and save to CSV\nresults_df = pd.DataFrame(results)\nresults_df.to_csv(\"/kaggle/working/youtube_comment_test_results_xlm_only.csv\", index=False)\nprint(\"Results saved to /kaggle/working/youtube_comment_test_results_xlm_only.csv\")\nprint(results_df)  # Display full results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\nfrom datasets import Dataset\n\n# Step 1: Load Excel file and convert to CSV\nexcel_file_path = \"/kaggle/input/random-multi-youtube-comments-scrapped/Youtube comments scrapper_Testing File.xlsx\"  # Replace with your actual file path\ncomments_df = pd.read_excel(excel_file_path)\n\n# Ensure the column name matches your Excel file (adjust if different)\nif \"comment\" not in comments_df.columns:\n    print(\"Column 'comment' not found. Available columns:\", comments_df.columns)\n    # Rename if needed, e.g., comments_df = comments_df.rename(columns={\"Comments\": \"comment_text\"})\nelse:\n    # Save as CSV\n    csv_file_path = \"/kaggle/working/comments.csv\"\n    comments_df[[\"comment\"]].to_csv(csv_file_path, index=False)\n    print(f\"Converted Excel to CSV at {csv_file_path} with {len(comments_df)} comments.\")\n\n# Step 2: Convert to Hugging Face Dataset\ncomments_dataset = Dataset.from_pandas(comments_df[[\"comment\"]])\n\n# Step 3: Tokenize with your fine-tuned XLM-RoBERTa tokenizer\nxlm_tokenizer = XLMRobertaTokenizer.from_pretrained(\"/kaggle/working/models/xlm_finetuned\")  # Load from your fine-tuned path\n\ndef tokenize_dataset(dataset, tokenizer, max_length=64):\n    def tokenize_function(examples):\n        return tokenizer(examples[\"comment\"], padding=\"max_length\", truncation=True, max_length=max_length)\n    return dataset.map(tokenize_function, batched=True)\n\ncomments_xlm = tokenize_dataset(comments_dataset, xlm_tokenizer)\n\n# Step 4: Load fine-tuned XLM-RoBERTa model\nxlm_model = XLMRobertaForSequenceClassification.from_pretrained(\"/kaggle/working/models/xlm_finetuned\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nxlm_model.to(device)\n\n# Step 5: Predict toxicity\ndef predict_toxicity(model, tokenizer, dataset, device):\n    model.eval()\n    predictions = []\n    probabilities = []\n    comments = dataset[\"comment\"]\n    for comment in comments:\n        inputs = tokenizer(comment, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n        inputs = {key: value.to(device) for key, value in inputs.items()}\n        with torch.no_grad():\n            outputs = model(**inputs)\n        probs = torch.softmax(outputs.logits, dim=-1)\n        pred = probs.argmax(-1).item()\n        toxic_prob = probs[0][1].item()\n        predictions.append(pred)\n        probabilities.append(toxic_prob)\n    return predictions, probabilities\n\nxlmr_preds, xlmr_probs = predict_toxicity(xlm_model, xlm_tokenizer, comments_dataset, device)\n\n# Step 6: Combine and save results\nresults_df = pd.DataFrame({\n    \"Comment\": comments_df[\"comment\"],\n    \"XLMR_Pred\": xlmr_preds,\n    \"XLMR_Toxic_Prob\": xlmr_probs\n})\nresults_csv_path = \"/kaggle/working/xlmr_comments_predictions.csv\"\nresults_df.to_csv(results_csv_path, index=False)\nprint(f\"Predictions saved to {results_csv_path}\")\nprint(results_df.head(10))  # Show first 10 rows for inspection","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T22:18:37.544435Z","iopub.execute_input":"2025-03-17T22:18:37.544753Z","iopub.status.idle":"2025-03-17T22:18:44.826559Z","shell.execute_reply.started":"2025-03-17T22:18:37.544730Z","shell.execute_reply":"2025-03-17T22:18:44.825478Z"}},"outputs":[{"name":"stdout","text":"Converted Excel to CSV at /kaggle/working/comments.csv with 563 comments.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/563 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93d266419dc4451d89e9e17aae6a60ea"}},"metadata":{}},{"name":"stdout","text":"Predictions saved to /kaggle/working/xlmr_comments_predictions.csv\n                                             Comment  XLMR_Pred  \\\n0         Learning to speak is always useful. Great!          0   \n1  Vocal Exercises begin at 7:50 :   1. Raise arm...          1   \n2  Imagine someone being late and walking in at 8:49          0   \n3  4:01 \"Tempered with love, honesty is a great t...          0   \n4  \"If you wish people with love its really hard ...          1   \n5  i feel like youtube recommendations are person...          0   \n6                      I like his head it is shining          1   \n7  Very fantastic video.‚ù§‚ù§  I came across this vi...          0   \n8  0:13 Intro 0:33 7 deadly sins of speaking 2:45...          0   \n9  The trick is, whenever you're talking to peopl...          1   \n\n   XLMR_Toxic_Prob  \n0         0.016863  \n1         0.915897  \n2         0.020808  \n3         0.016115  \n4         0.520468  \n5         0.018492  \n6         0.668092  \n7         0.015165  \n8         0.035361  \n9         0.825159  \n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\nfrom datasets import Dataset\n\n# Step 1: Load CSV file (instead of Excel) from Kaggle environment\ncsv_file_path = \"/kaggle/input/youtube-toxicity-data/youtoxic_english_1000.csv\"  # Replace with your actual CSV file path\ncomments_df = pd.read_csv(csv_file_path)\n\n# Ensure the column name matches your CSV file (adjust if different)\nif \"Text\" not in comments_df.columns:\n    print(\"Column 'comment' not found. Available columns:\", comments_df.columns)\n    # Rename if needed, e.g., comments_df = comments_df.rename(columns={\"Comments\": \"comment_text\"})\nelse:\n    print(f\"Loaded CSV from {csv_file_path} with {len(comments_df)} comments.\")\n\n# Step 2: Convert to Hugging Face Dataset\ncomments_dataset = Dataset.from_pandas(comments_df[[\"Text\"]])\n\n# Step 3: Tokenize with your fine-tuned XLM-RoBERTa tokenizer\nxlm_tokenizer = XLMRobertaTokenizer.from_pretrained(\"/kaggle/working/models/xlm_finetuned\")  # Load from your fine-tuned path\n\ndef tokenize_dataset(dataset, tokenizer, max_length=64):\n    def tokenize_function(examples):\n        return tokenizer(examples[\"Text\"], padding=\"max_length\", truncation=True, max_length=max_length)\n    return dataset.map(tokenize_function, batched=True)\n\ncomments_xlm = tokenize_dataset(comments_dataset, xlm_tokenizer)\n\n# Step 4: Load fine-tuned XLM-RoBERTa model\nxlm_model = XLMRobertaForSequenceClassification.from_pretrained(\"/kaggle/working/models/xlm_finetuned\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nxlm_model.to(device)\n\n# Step 5: Predict toxicity\ndef predict_toxicity(model, tokenizer, dataset, device):\n    model.eval()\n    predictions = []\n    probabilities = []\n    comments = dataset[\"Text\"]\n    for comment in comments:\n        inputs = tokenizer(comment, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n        inputs = {key: value.to(device) for key, value in inputs.items()}\n        with torch.no_grad():\n            outputs = model(**inputs)\n        probs = torch.softmax(outputs.logits, dim=-1)\n        pred = probs.argmax(-1).item()\n        toxic_prob = probs[0][1].item()\n        predictions.append(pred)\n        probabilities.append(toxic_prob)\n    return predictions, probabilities\n\nxlmr_preds, xlmr_probs = predict_toxicity(xlm_model, xlm_tokenizer, comments_dataset, device)\n\n# Step 6: Combine and save results\nresults_df = pd.DataFrame({\n    \"Comment\": comments_df[\"Text\"],\n    \"XLMR_Pred\": xlmr_preds,\n    \"XLMR_Toxic_Prob\": xlmr_probs\n})\nresults_csv_path = \"/kaggle/working/xlmr_YTD_comments_predictions.csv\"\nresults_df.to_csv(results_csv_path, index=False)\nprint(f\"Predictions saved to {results_csv_path}\")\nprint(results_df.head(10))  # Show first 10 rows for inspection\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T22:27:00.250918Z","iopub.execute_input":"2025-03-17T22:27:00.251270Z","iopub.status.idle":"2025-03-17T22:27:10.221228Z","shell.execute_reply.started":"2025-03-17T22:27:00.251244Z","shell.execute_reply":"2025-03-17T22:27:10.220399Z"}},"outputs":[{"name":"stdout","text":"Loaded CSV from /kaggle/input/youtube-toxicity-data/youtoxic_english_1000.csv with 1000 comments.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"172ac4a9fb0144ffb1883d83f4b5e694"}},"metadata":{}},{"name":"stdout","text":"Predictions saved to /kaggle/working/xlmr_YTD_comments_predictions.csv\n                                             Comment  XLMR_Pred  \\\n0  If only people would just take a step back and...          0   \n1  Law enforcement is not trained to shoot to app...          1   \n2  \\nDont you reckon them 'black lives matter' ba...          1   \n3  There are a very large number of people who do...          0   \n4  The Arab dude is absolutely right, he should h...          1   \n5  here people his facebook is¬†https://www.facebo...          1   \n6  Check out this you tube post. \"Black man goes ...          0   \n7  I would LOVE to see this pussy go to Staten Is...          1   \n8                        I agree with the protestor.          0   \n9   mike browns father was made to say that boooshit          1   \n\n   XLMR_Toxic_Prob  \n0         0.147595  \n1         0.997017  \n2         0.989839  \n3         0.057511  \n4         0.951912  \n5         0.989444  \n6         0.183976  \n7         0.997221  \n8         0.018221  \n9         0.993368  \n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\nfrom datasets import Dataset\n\n# Step 1: Load CSV file (instead of Excel) from Kaggle environment\ncsv_file_path = \"/kaggle/input/most-liked-comments-on-youtube/youtube_dataset.csv\"  # Replace with your actual CSV file path\ncomments_df = pd.read_csv(csv_file_path)\n\n# Ensure the column name matches your CSV file (adjust if different)\nif \"Comment\" not in comments_df.columns:\n    print(\"Column 'comment' not found. Available columns:\", comments_df.columns)\n    # Rename if needed, e.g., comments_df = comments_df.rename(columns={\"Comments\": \"comment_text\"})\nelse:\n    print(f\"Loaded CSV from {csv_file_path} with {len(comments_df)} comments.\")\n\n# Step 2: Convert to Hugging Face Dataset\ncomments_dataset = Dataset.from_pandas(comments_df[[\"Comment\"]])\n\n# Step 3: Tokenize with your fine-tuned XLM-RoBERTa tokenizer\nxlm_tokenizer = XLMRobertaTokenizer.from_pretrained(\"/kaggle/working/models/xlm_finetuned\")  # Load from your fine-tuned path\n\ndef tokenize_dataset(dataset, tokenizer, max_length=64):\n    def tokenize_function(examples):\n        return tokenizer(examples[\"Comment\"], padding=\"max_length\", truncation=True, max_length=max_length)\n    return dataset.map(tokenize_function, batched=True)\n\ncomments_xlm = tokenize_dataset(comments_dataset, xlm_tokenizer)\n\n# Step 4: Load fine-tuned XLM-RoBERTa model\nxlm_model = XLMRobertaForSequenceClassification.from_pretrained(\"/kaggle/working/models/xlm_finetuned\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nxlm_model.to(device)\n\n# Step 5: Predict toxicity\ndef predict_toxicity(model, tokenizer, dataset, device):\n    model.eval()\n    predictions = []\n    probabilities = []\n    comments = dataset[\"Comment\"]\n    for comment in comments:\n        inputs = tokenizer(comment, return_tensors=\"pt\", padding=True, truncation=True, max_length=64)\n        inputs = {key: value.to(device) for key, value in inputs.items()}\n        with torch.no_grad():\n            outputs = model(**inputs)\n        probs = torch.softmax(outputs.logits, dim=-1)\n        pred = probs.argmax(-1).item()\n        toxic_prob = probs[0][1].item()\n        predictions.append(pred)\n        probabilities.append(toxic_prob)\n    return predictions, probabilities\n\nxlmr_preds, xlmr_probs = predict_toxicity(xlm_model, xlm_tokenizer, comments_dataset, device)\n\n# Step 6: Combine and save results\nresults_df = pd.DataFrame({\n    \"Comment\": comments_df[\"Comment\"],\n    \"XLMR_Pred\": xlmr_preds,\n    \"XLMR_Toxic_Prob\": xlmr_probs\n})\nresults_csv_path = \"/kaggle/working/xlmr_MLCOY_comments_predictions.csv\"\nresults_df.to_csv(results_csv_path, index=False)\nprint(f\"Predictions saved to {results_csv_path}\")\nprint(results_df.head(10))  # Show first 10 rows for inspection\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T22:29:53.977143Z","iopub.execute_input":"2025-03-17T22:29:53.977463Z","iopub.status.idle":"2025-03-17T22:32:28.499126Z","shell.execute_reply.started":"2025-03-17T22:29:53.977438Z","shell.execute_reply":"2025-03-17T22:32:28.498328Z"}},"outputs":[{"name":"stdout","text":"Loaded CSV from /kaggle/input/most-liked-comments-on-youtube/youtube_dataset.csv with 19300 comments.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/19300 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f62cc94dfaca4784ad4a8238e4ec893f"}},"metadata":{}},{"name":"stdout","text":"Predictions saved to /kaggle/working/xlmr_MLCOY_comments_predictions.csv\n                                             Comment  XLMR_Pred  \\\n0  The people who liked this comment is officiall...          0   \n1           - Wait, it's 7B views\\n- Always has been          0   \n2  *Teacher: What is the population of the Earth?...          0   \n3  Let's be honest this wasn't your recommendatio...          0   \n4  Types Of People:\\n10% Enjoying Song \\n90% Chec...          0   \n5  3.2 Million comments if you find mine your a l...          0   \n6  claim your ‚Äúhere before 7 billion‚Äù tickets her...          0   \n7  The ones who are NOT from Tik-Tok can like thi...          0   \n8   Song: spanish\\nComments: English\\nHotel: trivago          0   \n9                                    Kimler burdaüòÇü•∞üåπ          0   \n\n   XLMR_Toxic_Prob  \n0         0.015564  \n1         0.016622  \n2         0.016170  \n3         0.016580  \n4         0.016175  \n5         0.456017  \n6         0.020753  \n7         0.017485  \n8         0.015184  \n9         0.307209  \n","output_type":"stream"}],"execution_count":13}]}